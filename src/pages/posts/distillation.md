---
layout: '../../layouts/MarkdownLayout.astro'
title: 'distillation'
author: 王福强
pubDate: 2023-09-09
keywords: distillation,蒸馏,王福强,Fuqiang Wang,fujohnwang
description: distillation - 架构百科
---

# distillation

今天有人在某个AI群里问“蒸馏”是什么意思？

在LLM（Large Language Model）中，"蒸馏（distillation）"是指模型压缩和优化的过程，以便使其更适合在计算资源有限的环境中运行。这个过程通常涉及到减小模型的大小，降低计算成本，去除冗余信息，以及优化模型的架构和参数等方面。

另外，在自然语言处理领域，"蒸馏（distillation）"有时也被用来描述一种将大型预训练模型的知识转移到小型模型的技术。这个过程通常涉及到使用大型模型的输出作为小型模型的输入，并根据这些输出对小型模型进行训练和微调，以便使其能够产生与大型模型相似的结果。比如，你可以用现在最牛逼的OpenAI的GPT模型的输出，作为你自己要训练的模型的输入，从而用更好的数据训练更好的垂直模型，是不是有点儿金融里follow trade的意思？ ；）
